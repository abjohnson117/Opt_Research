{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svdvals\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 40\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first set up the problem\n",
    "\\begin{align*}\n",
    "    Ax = b\n",
    "\\end{align*}\n",
    "where we $A$ is a random, standard Gaussian matrix of dimension $m \\times n$ (where the actual size of $m,n$ are defined in the cell immediately above), the true $x$ has 10 randomly selected spikes, and $b$ is given by\n",
    "\\begin{align*}\n",
    "    b = Ax + \\gamma y\n",
    "\\end{align*}\n",
    "where $y$ is a standard Gaussian noise vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.2\n",
    "A = np.random.normal(0,1,(m,n))\n",
    "x_true = np.zeros(n)\n",
    "idx_spike = np.random.randint(0,n,10)\n",
    "x_true[idx_spike] = 100\n",
    "b = np.dot(A,x_true) + gamma*np.random.normal(0,1,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize a first \"guess\" for $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = np.ones(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find a potential answer to our inverse problem and solve for some $x$. To do this, we first calculate the $\\lambda$ hyperparameter used in the regularized linear least squares problem:\n",
    "\\begin{align*}\n",
    "     \\displaystyle \\min_{x} \\frac{1}{2}||Ax - b||^{2} + \\lambda R(Cx).\n",
    "\\end{align*}\n",
    "Since we are first implementing the lasso method, we know necessarily that $R(Cx) = ||x||_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = np.linalg.norm(np.dot(A.T, b),ord=np.inf)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement the Lasso method. Since Lasso necessarily uses the $||\\cdot ||_1$ norm, then we define \n",
    "\\begin{align}\n",
    "    \\mathrm{prox}_{\\lambda || \\cdot ||_1}(x) = \\mathrm{sign}(x)\\mathrm{max}(|x|-\\lambda,0)\n",
    "\\end{align}\n",
    "Note, it is necessary that $x \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a maximum function that compares every element in a $\\verb+numpy+$ array to a desired number (of type float or int) and returns a vector of like dimension that has taken the maximum value between the desired number and every element in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_function(x, num_compare):\n",
    "    return np.array([i if i >= num_compare else num_compare for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function implements formula (1). Note that every operation is completed entry-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_op(x,lambd):\n",
    "    return np.sign(x)*max_function(np.abs(x)-lambd,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_A = svdvals(A)[0]\n",
    "eta = 0.001\n",
    "k = 0\n",
    "tol = 0.00001\n",
    "max_step = 100\n",
    "assert eta <= 1/(cond_A**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a $\\verb+lasso+$ function that will perform the desired prox-gradient descent and solve the inverse problem constructed, with the intent of creating an $\\verb+x_guess+$ vector that is as close to $\\verb+x_true+$ as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(x, max_step, k, eta, lam, tol):\n",
    "    start = time.time()\n",
    "    while (max_step > tol):\n",
    "        k += 1\n",
    "        x_old = x\n",
    "        x = prox_op(x_old - eta*(A.T)@((A@x_old)-b),eta*lam)\n",
    "        step = abs(x - x_old)\n",
    "        max_step = step.max()\n",
    "    end = time.time()\n",
    "    return x, start, end, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to run our $\\verb+lasso+$ function and see if, at the very least, it includes the nonzero entries inside our $\\verb+x_true+$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess, start, end, iters = lasso(x_init, max_step, k, eta, lam, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_guess_nonzero = np.nonzero(x_guess)\n",
    "x_true_nonzero = np.nonzero(x_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isin(x_true_nonzero, x_guess_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what is directly above, we see clearly that all the nonzero entries inside our true solution $\\verb+x_true+$ are at least inside the solution given by the lasso regression, $\\verb+x_guess+$. Though, note that the number of nonzero entries in $\\verb+x_guess+$ is greater than that of $\\verb+x_true+$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of nonzero entries in x_guess: 16\n",
      "This is the number of nonzero entries in x_true: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the number of nonzero entries in x_guess: \" + str(len(x_guess_nonzero[0])))\n",
    "print(\"This is the number of nonzero entries in x_true: \" + str(len(x_true_nonzero[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, there is imprecision in that we would prefer to have more entries in $\\verb+x_guess+$ to be closer to 0.\n",
    "\n",
    "We also show the number of iterations and the time it takes for the lasso method to converge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of iterations required for lasso to converge: 2354\n",
      "This is the number of seconds it takes for lasso to converge: 0.17475485801696777\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the number of iterations required for lasso to converge: \" + str(iters))\n",
    "print(\"This is the number of seconds it takes for lasso to converge: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare the above to the SR3 method. For reference, the general SR3 formulation is as follows:\n",
    "\\begin{align*}\n",
    "    \\displaystyle \\min_{x,w} \\frac{1}{2}||Ax-b||^{2} + \\lambda R(w) + \\frac{\\kappa}{2}||Cx - w||^{2}.\n",
    "\\end{align*}\n",
    "We first want to recover a relaxed version of lasso. To do this, we take $R(\\cdot) = ||\\cdot ||_{1}$ and $C=I$. In this instance as well, we take $\\kappa = 100$. We proceed as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.eye(n,n)\n",
    "kappa = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to the algorithm we implement is\n",
    "\\begin{align*}\n",
    "    H_{\\kappa} = A^{\\top}A + \\kappa C^{\\top}C .\n",
    "\\end{align*}\n",
    "We define this matrix immediately below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_k = np.dot(A.T,A) + kappa*np.dot(C.T,C)\n",
    "H = np.linalg.inv(H_k)\n",
    "w = np.ones(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement the algorithm for SR3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sr3 = 0\n",
    "eta = 1/kappa\n",
    "max_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr3(w, H, A, b, C, max_step, k, eta, tol, kappa, lam):\n",
    "    start = time.time()\n",
    "    while (max_step > tol):\n",
    "        k += 1\n",
    "        w_old = w\n",
    "        x = H@(np.dot(A.T, b) + kappa*np.dot(C.T, w_old))\n",
    "        w = prox_op(np.dot(C,x),eta*lam)\n",
    "        step = abs(w - w_old)\n",
    "        max_step = step.max()\n",
    "    end = time.time()\n",
    "    return w, start, end, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_guess, start_sr3, end_sr3, iters_sr3 = sr3(w,H,A,b,C,max_step,k_sr3,eta,tol,kappa,lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, False,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_guess_nonzero = np.nonzero(w_guess)\n",
    "np.isin(x_true_nonzero, w_guess_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, $\\verb+w_guess+$ contains nonzero entries in mostly the same places as does $\\verb+x_true+$. Next, we look at how many nonzero entries there are in $\\verb+w_guess+$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of nonzero entries in w_guess: 12\n",
      "This is the number of nonzero entries in x_true: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the number of nonzero entries in w_guess: \" + str(len(w_guess_nonzero[0])))\n",
    "print(\"This is the number of nonzero entries in x_true: \" + str(len(x_true_nonzero[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, even though there are more nonzero entries in $\\verb+w_guess+$ than in $\\verb+x_true+$, the number of non-sparse entries is much closer than what was obtained using the standard lasso method, using all the same parameters. Thus, we have a higher level of precision, as we desired.\n",
    "\n",
    "Last, we look at the number of iterations SR3 took to converge and the number of seconds required fro convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of iterations required for SR3 to converge: 148\n",
      "This is the number of seconds it takes for SR3 to converge: 0.3275918960571289\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the number of iterations required for SR3 to converge: \" + str(iters_sr3))\n",
    "print(\"This is the number of seconds it takes for SR3 to converge: \" + str(end_sr3 - start_sr3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, even for a small-dimension problem, the number of iterations required for SR3 to converge is decreased by an order of 10, as for the number of seconds required for convergence. This is exactly what we anticipated from the results shown in J33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we seek to deblur an image using SR3, implementing the FISTA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
